---
title: "Lab 9"
author: "Dan Lucaciu Chifu"
date: "15/03/2021"
output: html_document
---

## Neural networks (seeds data)
+ seeds data set from UCI repository1

+ Explore the data.

  + Measurements of geometrical properties of kernels belonging to three different varieties of wheat. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.


```{r message=FALSE}
library(tidyverse)
library(kableExtra)
```

```{r}
seeds <- read.table(
  "https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt"
  )
colnames(seeds) <- c("area", 
                     "perimeter", 
                     "compactness", 
                     "length_of_kernel", 
                     "width_of_kernel",
                     "asy_coeff", 
                     "length_of_kernel_groove", 
                     "Class")
summary(seeds)
cor(dplyr::select(seeds, -Class))
```

+ Print tidy seeds data set.

```{r}
dim(seeds)
knitr::kable(head(seeds)) %>%
  kable_styling(latex_options="scale_down")
```

+ We scale the predictors.

```{r}
x <- seeds %>%
  dplyr::select(-Class) %>%
  scale()
```

+ We split 75%/25% training/test set.

```{r}
set.seed(1)

seeds_train_index <- seeds %>%
  mutate(ind = 1:nrow(seeds)) %>%
  group_by(Class) %>%
  mutate(n = n()) %>%
  sample_frac(size = .75, weight = n) %>%
  ungroup() %>%
  pull(ind)
```

+ We create binary output units yk,k=1,2,3 using class.ind().

```{r}
library(nnet)
class_labels <- pull(seeds, Class) %>% 
  class.ind() 
knitr::kable(head(class_labels)) %>%
  kable_styling(latex_options="scale_down")
```

+ Create predictor matrix for training/test set and output for training/test set.

```{r}
seeds_train <- x[seeds_train_index, ]
train_class <- class_labels[seeds_train_index,]
seeds_test <- x[-seeds_train_index, ] 
test_class <- class_labels[-seeds_train_index,]
```


+ Let’s look at the help page for nnet().

+ Let’s tune size = number of units in the hidden layer and decay = weight decay parameter.

```{r}
nn_seeds <- nnet(
  x = seeds_train, 
  y = train_class, 
  size = 4, 
  decay = 0, 
  softmax = TRUE,
  maxit=500
  )
```

+ Compute test error for NN with size = 4 and decay = 0.

```{r}
nn_pred <- predict(nn_seeds, seeds_test, 
                   type="class")

tab_seeds <- table(slice(
  seeds, 
  -seeds_train_index) %>% pull(Class), 
  nn_pred)

1-sum(diag(tab_seeds))/sum(tab_seeds)
```

## Neural Networks (Boston Data (quantitative response))
+ Let’s consider housing price data, Boston in MASS package.
+ Response is quantitative.

```{r message=FALSE}
library(nnet)
library(MASS)

```

+ Scale predictor, response, split training/test set

```{r}
train_Boston <- sample(
  1:nrow(Boston), 
  nrow(Boston)/2
  )

x <- scale(Boston)
```

+ Create predictor matrix for training/test set and output for training/test set.

```{r}
Boston_train <- x[train_Boston, ]
train_medv <- x[train_Boston, "medv"]
Boston_test <- x[-train_Boston, ] 
test_medv <- x[-train_Boston, "medv"]
```

+ Tune size = number of units in the hidden layer and decay = weight decay parameter.

```{r}
nn_Boston <- nnet(
  Boston_train, 
  train_medv,  
  size=10, 
  decay=1, 
  softmax=FALSE, 
  maxit=1000,
  linout=TRUE
  )
```

+ Compute test error for the above model: NN with size = 10 and decay = 1.

```{r}
nn_pred <- predict(
  nn_Boston, 
  Boston_test,
  type="raw"
  )
```

+ Visualize the results

```{r}
plot(test_medv, nn_pred)

mean((test_medv - nn_pred)^2)
```

## CV for NN - Iris Data

+ 80% / 20% training/test data

```{r}
library(e1071)
library(cluster)
set.seed(1)

data("iris")

Species <- pull(iris, Species)

xy <- dplyr::select(iris, -Species) %>%
  scale() %>% 
  data.frame() %>% 
  mutate(Species = Species) # scale predictors

iris_train_index <- iris %>%
  mutate(ind = 1:nrow(iris)) %>%
  group_by(Species) %>%
  mutate(n = n()) %>%
  sample_frac(size = .8, weight = n) %>%
  ungroup() %>%
  pull(ind)

iris_train <- slice(xy, iris_train_index)
iris_test <- slice(xy, -iris_train_index)
class_labels <- pull(xy, Species) %>% 
  class.ind() 

iris_nnet1 <- tune.nnet(
  Species~., 
  data = iris_train, 
  size = 1:30, 
  tunecontrol = tune.control(sampling = "cross",cross=5)
  )

head(summary(iris_nnet1))

plot(iris_nnet1)
```

+ Perform model selection and fit the model (adapt the parameter size)

```{r}
library(nnet)
nn_iris <- nnet(
  x = dplyr::select(iris_train, -Species),
  y = class_labels[iris_train_index, ],
  size = iris_nnet1$best.parameters[1,1], 
  decay = 0, 
  softmax = TRUE
  )
```

+ Compute test error for the selected model

```{r}
nn_pred <- predict(
  nn_iris, 
  dplyr::select(iris_test, -Species), 
  type="class"
  )

tab <- table(pull(iris_test, Species), 
  nn_pred
  )

tab
1- sum(diag(tab))/sum(tab)
```

+ Tune decay and size (Iris Data)

```{r}
set.seed(1)

iris_nnet2 <- tune.nnet(
  Species~., 
  data = iris_train, 
  size = 1:20,
  decay = 0:3,
  tunecontrol = tune.control(sampling = "cross",cross=5)
  )

head(summary(iris_nnet2))

plot(iris_nnet2)
```

+ Fit the model with __size = iris_nnet2$best.parameters[1,1] and decay = iris_nnet2$best.parameters[1,2]__

```{r}
nn_iris_d_s <- nnet(
  x = dplyr::select(iris_train, -Species),
  y = class_labels[iris_train_index, ], 
  size = iris_nnet2$best.parameters[1,1], 
  decay = iris_nnet2$best.parameters[1,2], 
  softmax = TRUE
  )

# Compute test error
nn_pred <- predict(
  nn_iris_d_s, 
  dplyr::select(iris_test, -Species), 
  type="class"
  )

tab <- table(pull(iris_test, Species), 
  nn_pred
  )

tab
1- sum(diag(tab))/sum(tab)
```

## Clustering (Coffee Data)

+ Apply k-means clustering to the coffee data

```{r}
library(cluster) 
library(factoextra) # PCA
library(pgmm) # coffee data
data("coffee")
set.seed(1)
x <- dplyr::select(coffee, - Variety, - Country) 
x_scaled <- scale(x)
kmeans_coffee <- kmeans(x_scaled, 2)
kmeans_coffee$tot.withinss
kmeans_coffee <- kmeans(x_scaled, 3)
kmeans_coffee$tot.withinss

# Let's select K using elbow method
withiclusterss <- function(K,x){
  kmeans(x, K)$tot.withinss
}

K <- 1:8

wcss <- lapply(as.list(K), function(k){
  withiclusterss(k, x_scaled)
}) %>% unlist()

ggplot(tibble(K = K, wcss = wcss), aes(x = K, y = wcss)) +
  geom_point() +
  geom_line() +
  xlab("Number of clusters (k)") +
  ylab("Total within-clusters sum of squares") +
  scale_x_continuous(breaks=c(seq(1,K[length(K)])))
```

+ Based on the elbow method, we can use k=2. There are __dim(x_scaled)[2]__ variables. So we need to use dimensionality reduction technique and then plot the clusters in two-dimensions.

```{r}
kmeans_coffee <- kmeans(x_scaled, 2)
fvPCA <- fviz_cluster(kmeans_coffee, 
                    x_scaled, 
                    ellipse.type = "norm",
                    main = "Plot the results of k-means clustering after PCA")
fvPCA
```

+ Use silhouette plots to choose best clusters (choose k = 2)

```{r}
si <- silhouette(kmeans_coffee$cluster, dist(x_scaled))
head(si)
#average Silhouette width
mean(si[, 3])
plot(si, nmax= 80, cex.names=0.6, main = "")

# Let's select K using average Silhouette width
avgSilhouette <- function(K,x) {
  km_cl <- kmeans(x, K)
  sil <- silhouette(km_cl$cluster, dist(x)) 
  return(mean(sil[, 3]))
}

K <- 2:8

avgSil <- numeric()
for(i in K){
  avgSil[(i-1)] <- avgSilhouette(i, x_scaled)
}

ggplot(tibble(K = K, avgSil = avgSil), aes(x = K, y = avgSil)) +
  geom_point() +
  geom_line() +
  xlab("Number of clusters (k)") +
  ylab("Average silhouette width") +
  scale_x_continuous(breaks=c(seq(1,K[length(K)])))
```

+ Apply k-medoids to coffee data

```{r}
kmedoid_coffee <- pam(x_scaled, 2)
kmedoid_coffee$silinfo$avg.width

avgSil <- lapply(as.list(2:8), function(k){
  kmedoid_coffee <- pam(x_scaled, k)
kmedoid_coffee$silinfo$avg.width
}) %>% unlist()

ggplot(tibble(K = 2:8, avgSil = avgSil), aes(x = K, y = avgSil)) +
  geom_point() +
  geom_line() +
  xlab("Number of clusters (k)") +
  ylab("Average silhouette width for k-medoid") +
  scale_x_continuous(breaks=c(seq(1,K[length(K)])))
```

## Clustering - votes data

# Heirarchical clustering - divisive clustering

+ Divisive clustering - look for most disparate observation which initiates the splinter group. In the next steps, the algorith reassigns observations that are closer to the splinter group

```{r}
library(cluster)
data(votes.repub) # from cluster package

library(factoextra)
divisive_votes <- diana(
  votes.repub, 
  metric = "euclidean", 
  stand = TRUE
  )

plot(divisive_votes)

cut_divisive_votes <- cutree(as.hclust(divisive_votes), k = 2)
table(cut_divisive_votes) # 8 and 42 group members
rownames(votes.repub)[cut_divisive_votes == 1]
# rownames(votes.repub)[cut_divisive_votes == 2]

#make a nice dendrogram
fviz_dend(
  divisive_votes, 
  cex = 0.5,
  k = 2, # Cut in 2 groups
  palette = "jco", # Color palette
  main = "Dendrogram for votes data (divisive clustering)")

```


+ Agglomerative clustering

```{r}
x <- votes.repub %>% 
  scale()
hc_vote <- hclust(dist(x), "ward.D")
plot(hc_vote)


#make a nice dendrogram
fviz_dend(
  hc_vote, 
  k = 2, # Cut in 2 groups
  cex = 0.5, 
  color_labels_by_k = TRUE, 
  rect = TRUE,
  main = "Dendrogram for votes data (agglomerative clustering)"
  )
```

